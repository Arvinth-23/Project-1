{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b292032c",
   "metadata": {},
   "source": [
    "# üìò LangChain + Google Gemini\n",
    "# AI Study Helper ‚Äì Full LangChain Concepts (Teaching Version)\n",
    "\n",
    "This notebook demonstrates ALL CORE LangChain CONCEPTS before moving to LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4519bc",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62d1fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain (c:\\Users\\senth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (c:\\Users\\senth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain (c:\\Users\\senth\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages with latest versions\n",
    "# %pip: Jupyter magic command to run pip in the notebook kernel\n",
    "# -q: quiet mode (suppresses package download progress)\n",
    "# langchain: Core LangChain framework for building chains and agents\n",
    "# langchain-core: Core abstractions for LangChain components\n",
    "# langchain-google-genai: Integration with Google's Generative AI models\n",
    "# google-generativeai: Google's Python SDK for their generative models\n",
    "# langchain-community: Community-maintained LangChain integrations\n",
    "%pip install -q langchain langchain-core langchain-google-genai google-generativeai langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01414af4",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Set API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb53ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os module for environment variable management\n",
    "import os\n",
    "\n",
    "# Set your Google API key here - this authenticates all API calls to Google's Gemini models\n",
    "# Get your key from: https://makersuite.google.com/app/apikey\n",
    "# The key is stored in environment variable GOOGLE_API_KEY for secure access\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "# Verify API key is set (will show *** if set)\n",
    "# This is optional - uncomment to check if API key is configured\n",
    "#if os.environ.get(\"GOOGLE_API_KEY\") and os.environ.get(\"GOOGLE_API_KEY\") != \"YOUR_GEMINI_API_KEY\":\n",
    "#    print(\"‚úÖ Google API Key is configured\")\n",
    "#else:\n",
    "#    print(\"‚ö†Ô∏è Please set your GOOGLE_API_KEY in this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558be594",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Import LangChain Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3460764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ChatGoogleGenerativeAI: LLM wrapper for Google's Generative AI models (like Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Import PromptTemplate: Framework for creating dynamic prompts with variable placeholders\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Import StrOutputParser: Converts LLM output (AIMessage objects) to plain strings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Import runnable components - building blocks for creating data pipelines:\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,    # Passes input data through unchanged (identity operation)\n",
    "    RunnableLambda,         # Wraps custom Python functions into runnable components\n",
    "    RunnableParallel        # Executes multiple runnables simultaneously and combines outputs\n",
    ")\n",
    "\n",
    "# Import StructuredTool: For creating tools with validated input/output schemas\n",
    "from langchain_core.tools import StructuredTool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27614caf",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Initialize LLM (Brain of the System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fac5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ChatGoogleGenerativeAI - this is the \"brain\" of our AI system\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # Specify which Gemini model to use (fast and efficient)\n",
    "    temperature=0.3            # Temperature controls randomness: 0=deterministic, 1=very random\n",
    "                               # 0.3 = balanced (consistent but still creative)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8154ca1a",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Prompt Templates (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5226ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first prompt template for explaining topics\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],  # Define the variable name that will be filled in: {topic}\n",
    "    template=\"Explain {topic} in very simple terms for a college student.\"  # Template with placeholder\n",
    ")\n",
    "\n",
    "# Create second prompt template for generating quiz questions\n",
    "quiz_prompt = PromptTemplate(\n",
    "    input_variables=[\"explanation\"],  # Define the variable: {explanation}\n",
    "    template=\"\"\"\n",
    "Based on the explanation below, create 3 simple quiz questions:\n",
    "\n",
    "{explanation}\n",
    "\"\"\"  # Multi-line template with placeholder for the explanation content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb0613",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Output Parser (Production-Safe Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1a4565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output parser to convert LLM responses to plain text strings\n",
    "# Without this, LLM returns AIMessage objects with metadata\n",
    "# StrOutputParser extracts just the text content from the response\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c233d1d7",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Chains Using LCEL (| Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "424c38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains using LCEL (LangChain Expression Language)\n",
    "# The | operator chains components: input ‚Üí prompt ‚Üí llm ‚Üí parser ‚Üí output\n",
    "\n",
    "# First chain: explain_prompt feeds to llm, then result goes to parser\n",
    "# Flow: {topic} ‚Üí explain_prompt.format() ‚Üí llm.invoke() ‚Üí parser.invoke()\n",
    "explain_chain = explain_prompt | llm | parser\n",
    "\n",
    "# Second chain: quiz_prompt feeds to llm, then result goes to parser\n",
    "# Flow: {explanation} ‚Üí quiz_prompt.format() ‚Üí llm.invoke() ‚Üí parser.invoke()\n",
    "quiz_chain = quiz_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1829ae3",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Sequential Workflow (Explain ‚Üí Quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11761e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential workflow that chains multiple operations together\n",
    "study_helper_chain = (\n",
    "    {\"topic\": RunnablePassthrough()}     # First: Take input as-is and pass it as 'topic' key\n",
    "    | explain_chain                      # Second: Send to explain_chain (gets explanation)\n",
    "    | quiz_chain                         # Third: Pass explanation result to quiz_chain (gets quiz)\n",
    ")\n",
    "# This creates a complete workflow: Input Topic ‚Üí Explanation ‚Üí Quiz Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee8440",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Run Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e287b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study Helper Result:\n",
      "Here are 3 simple quiz questions based on the explanation:\n",
      "\n",
      "1.  Based on the analogy, what is the Operating System (OS) compared to in a bustling city or busy hotel?\n",
      "2.  When you open a program like a game or a word processor, what does the OS do to help it run, similar to a hotel manager checking a guest into a room?\n",
      "3.  According to the explanation, what would your computer essentially be without an Operating System (OS)?\n"
     ]
    }
   ],
   "source": [
    "# Run the sequential chain with error handling\n",
    "try:\n",
    "    # invoke() executes the chain with the given input\n",
    "    # The input flows through: topic ‚Üí explain ‚Üí quiz\n",
    "    result = study_helper_chain.invoke(\"Operating System\")\n",
    "    \n",
    "    # Print the result (this will contain the quiz questions)\n",
    "    print(\"Study Helper Result:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Catch any errors (usually from invalid API key)\n",
    "    print(f\"‚ö†Ô∏è Note: Chain execution requires a valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")  # Print the type of error that occurred\n",
    "    print(f\"Chains are properly constructed. Add your API key in cell 4 to run this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafcf7b",
   "metadata": {},
   "source": [
    "## üîü Memory (Stateful Conversation)\n",
    "\n",
    "### Concepts Covered\n",
    "- Short-term memory\n",
    "- Context retention\n",
    "- Stateful AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ec5b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory Concept Demonstration:\n",
      "==================================================\n",
      "User: Explain Operating System\n",
      "AI: [AI Response about: Explain Operating System]\n",
      "\n",
      "User: Now explain scheduling in OS\n",
      "AI: [AI Response about: Now explain scheduling in OS]\n",
      "\n",
      "üìã Complete Conversation History:\n",
      "1. USER: Explain Operating System\n",
      "2. ASSISTANT: [AI Response about: Explain Operating System]\n",
      "3. USER: Now explain scheduling in OS\n",
      "4. ASSISTANT: [AI Response about: Now explain scheduling in OS]\n",
      "\n",
      "‚úÖ Memory demonstrates how AI retains context across multiple interactions\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate memory concept using a simple list to store conversation history\n",
    "# Modern LangChain moved memory components - using manual approach for demonstration\n",
    "\n",
    "# Create a simple conversation history storage\n",
    "conversation_history = []\n",
    "\n",
    "# Define a function to simulate stateful conversation\n",
    "def stateful_conversation(question: str, memory: list) -> str:\n",
    "    \"\"\"Simulates a conversation that remembers context\"\"\"\n",
    "    # Store question in memory\n",
    "    memory.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    # Create context from memory\n",
    "    context = \"Previous conversation:\\n\"\n",
    "    for msg in memory[-3:]:  # Keep last 3 messages for context\n",
    "        context += f\"- {msg['role']}: {msg['content']}\\n\"\n",
    "    \n",
    "    # Simulate LLM response (in real usage, this would call llm.invoke())\n",
    "    response = f\"[AI Response about: {question}]\"\n",
    "    memory.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"üß† Memory Concept Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First interaction: Explain OS\n",
    "response1 = stateful_conversation(\"Explain Operating System\", conversation_history)\n",
    "print(f\"User: Explain Operating System\")\n",
    "print(f\"AI: {response1}\\n\")\n",
    "\n",
    "# Second interaction: Follow-up question (with context retained)\n",
    "response2 = stateful_conversation(\"Now explain scheduling in OS\", conversation_history)\n",
    "print(f\"User: Now explain scheduling in OS\")\n",
    "print(f\"AI: {response2}\\n\")\n",
    "\n",
    "# Show the complete conversation history\n",
    "print(\"üìã Complete Conversation History:\")\n",
    "for i, msg in enumerate(conversation_history, 1):\n",
    "    print(f\"{i}. {msg['role'].upper()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Memory demonstrates how AI retains context across multiple interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68de1f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Tools (Action Capability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5abf7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tool decorator from langchain_core for creating tools\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define a tool using the @tool decorator (modern recommended approach)\n",
    "# Tools are functions that AI agents can call to take actions\n",
    "@tool\n",
    "def syllabus_lookup(topic: str) -> str:\n",
    "    \"\"\"Fetch syllabus details for a subject\"\"\"\n",
    "    # This is a simple mock function - in reality, it could query a database\n",
    "    return f\"Syllabus for {topic}: Basics, Architecture, Scheduling, Memory Management, I/O Systems.\"\n",
    "\n",
    "# Alternative approach using Tool class directly (commented out):\n",
    "# from langchain_core.tools import Tool\n",
    "# This would be used if you need more control over tool configuration\n",
    "# syllabus_tool = Tool(\n",
    "#     name=\"SyllabusSearch\",\n",
    "#     func=syllabus_lookup,\n",
    "#     description=\"Fetch syllabus details for a subject\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a111d99",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Agents (Decision-Making AI)\n",
    "\n",
    "### üìå This is where AI becomes Agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d1fbcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGENTS: Reasoning with Tools ===\n",
      "\n",
      "Modern agent imports not available, using simple agent demo\n",
      "\n",
      "Using Simplified Agent Demonstration:\n",
      "\n",
      "[TeachingAssistant] Received query: What topics are covered in Operating System syllabus? Explain the key concepts.\n",
      "[Agent Reasoning] YES. The user is asking for a list of topics and key concepts from an Operating System syllabus. The `syllabus_lookup` tool is designed to retrieve this exact type of information....\n",
      "\n",
      "[Agent Action] Using tool: syllabus_lookup\n",
      "[Tool Result] Retrieved syllabus data\n",
      "\n",
      "\n",
      "[Final Response]\n",
      "Based on the syllabus information provided, an Operating System (OS) syllabus typically covers the following core topics, each with its own set of key concepts:\n",
      "\n",
      "### What topics are covered in Operating System syllabus?\n",
      "\n",
      "The syllabus covers the fundamental aspects of how an operating system works, manages resources, and interacts with hardware and applications. The key topics are:\n",
      "\n",
      "1.  **Basics**\n",
      "2.  **Architecture**\n",
      "3.  **Scheduling**\n",
      "4.  **Memory Management**\n",
      "5.  **I/O Systems**\n",
      "\n",
      "### Explain the key concepts for each topic:\n",
      "\n",
      "---\n",
      "\n",
      "#### 1. Basics\n",
      "\n",
      "This foundational section introduces what an operating system is, its purpose, and its fundamental role in a computer system.\n",
      "\n",
      "*   **Definition and Role of an OS:** An OS is system software that manages computer hardware and software resources and provides common services for computer programs. It acts as an intermediary between the user/applications and the hardware.\n",
      "*   **Goals of an OS:** To provide convenience (making the computer easier to use), efficiency (managing resources effectively), and the ability to evolve (allowing for new features and hardware).\n",
      "*   **System Calls:** The interface between user-level applications and the operating system. Programs request services from the OS (e.g., file access, memory allocation) via system calls.\n",
      "*   **Operating System Services:** Functions provided by the OS to both the user and the system, such as program execution, I/O operations, file system manipulation, communication, error detection, and resource allocation.\n",
      "*   **Kernel:** The core part of the operating system that manages the system's resources and provides the lowest-level services. It's the first part of the OS to load and remains in memory.\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. Architecture\n",
      "\n",
      "This topic explores the different ways an operating system can be structured internally, influencing its design, performance, and security.\n",
      "\n",
      "*   **Monolithic Kernel:** A traditional architecture where the entire operating system runs in a single address space in kernel mode. All OS services (process management, memory management, file system, device drivers) are part of the kernel.\n",
      "    *   **Key Concept:** High performance due to direct communication, but less modular and harder to maintain/debug.\n",
      "*   **Layered Approach:** The OS is broken into layers, where each layer uses services provided by the layer below it and provides services to the layer above.\n",
      "    *   **Key Concept:** Modularity and easier debugging, but can suffer from performance overhead due to multiple layer traversals.\n",
      "*   **Microkernel:** A minimal kernel that provides only essential services like inter-process communication (IPC), memory management, and basic scheduling. Most other OS services (file systems, device drivers, network protocols) run as user-level processes.\n",
      "    *   **Key Concept:** Enhanced modularity, security, and extensibility, as failures in user-level services don't crash the entire kernel. Can have performance overhead due to IPC.\n",
      "*   **Hybrid Systems:** Modern operating systems often combine aspects of different architectures (e.g., macOS and Windows use hybrid kernels that have some microkernel features but keep many services in the kernel for performance).\n",
      "    *   **Key Concept:** A pragmatic approach to balance performance, modularity, and security.\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. Scheduling\n",
      "\n",
      "Scheduling deals with how the operating system manages and allocates the CPU (and other resources) among multiple competing processes or threads.\n",
      "\n",
      "*   **Process Concept:** A program in execution. It includes the program code, its current activity (program counter, registers), and its stack, data section, and heap.\n",
      "*   **Process States:** The different phases a process goes through during its lifetime (e.g., New, Ready, Running, Waiting, Terminated).\n",
      "*   **Context Switching:** The mechanism by which the OS saves the state of a currently running process and loads the state of another process to be run. This allows multiple processes to share a single CPU.\n",
      "*   **CPU Scheduler (Short-Term Scheduler):** Selects which process in the ready queue should be executed next by the CPU.\n",
      "*   **Scheduling Algorithms:** Different strategies to decide which process gets the CPU and for how long.\n",
      "    *   **First-Come, First-Served (FCFS):** Processes are executed in the order they arrive.\n",
      "    *   **Shortest-Job-First (SJF):** The process with the smallest estimated execution time is run next. Can be preemptive or non-preemptive.\n",
      "    *   **Priority Scheduling:** Processes are assigned a priority, and the highest-priority process runs first.\n",
      "    *   **Round Robin (RR):** Each process gets a small unit of CPU time (time quantum) in a cyclic manner.\n",
      "    *   **Multilevel Queue Scheduling:** Ready queue is partitioned into separate queues (e.g., foreground/interactive, background/batch), each with its own scheduling algorithm.\n",
      "    *   **Multilevel Feedback Queue Scheduling:** Allows processes to move between queues based on their CPU burst characteristics, preventing starvation and favoring interactive processes.\n",
      "*   **Performance Criteria:** Metrics used to evaluate scheduling algorithms, such as CPU utilization, throughput, turnaround time, waiting time, and response time.\n",
      "\n",
      "---\n",
      "\n",
      "#### 4. Memory Management\n",
      "\n",
      "This topic focuses on how the operating system manages the computer's main memory (RAM), allocating it to processes and ensuring efficient and secure access.\n",
      "\n",
      "*   **Logical vs. Physical Address Space:**\n",
      "    *   **Logical Address:** The address generated by the CPU (virtual address).\n",
      "    *   **Physical Address:** The actual address in main memory.\n",
      "    *   **Key Concept:** Memory Management Unit (MMU) maps logical addresses to physical addresses, allowing programs to run without knowing the physical layout of memory.\n",
      "*   **Memory Allocation:** Strategies for assigning memory to processes.\n",
      "    *   **Contiguous Allocation:** Each process is allocated a single, contiguous block of memory.\n",
      "    *   **Non-Contiguous Allocation:** A process's memory can be spread across different, non-adjacent physical blocks.\n",
      "*   **Paging:** A non-contiguous memory allocation scheme where physical memory is divided into fixed-size blocks called **frames**, and logical memory is divided into same-sized blocks called **pages**. The OS maps pages to frames.\n",
      "    *   **Key Concept:** Eliminates external fragmentation, enables virtual memory.\n",
      "*   **Segmentation:** A memory management scheme that supports the user's view of memory. Programs are divided into logical units called **segments** (e.g., code, data, stack), which can be of varying sizes.\n",
      "    *   **Key Concept:** Reflects the logical structure of a program, but can lead to external fragmentation.\n",
      "*   **Virtual Memory:** A technique that allows a program to execute even if it's not entirely in physical memory. It uses disk space as an extension of RAM.\n",
      "    *   **Key Concept:** Allows programs larger than physical memory to run, increases multiprogramming, and simplifies programming.\n",
      "*   **Demand Paging:** A virtual memory technique where pages are loaded into memory only when they are actually needed (on demand).\n",
      "    *   **Key Concept:** Reduces I/O and memory requirements, but introduces page faults.\n",
      "*   **Page Replacement Algorithms:** When a page fault occurs and memory is full, an algorithm decides which page to remove from memory to make space for the new page (e.g., FIFO, LRU, Optimal).\n",
      "*   **Thrashing:** A situation where a system spends most of its time swapping pages in and out of memory rather than executing application code, leading to very low CPU utilization.\n",
      "\n",
      "---\n",
      "\n",
      "#### 5. I/O Systems\n",
      "\n",
      "This section covers how the operating system manages input and output operations, interacting with various hardware devices.\n",
      "\n",
      "*   **I/O Hardware:** Understanding the components involved in I/O, such as devices (keyboard, mouse, disk), device controllers (electronic circuits that operate a device), and I/O ports.\n",
      "*   **I/O Software:** The software layers that manage I/O operations, including:\n",
      "    *   **Device Drivers:** Software modules specific to a particular device that translate OS requests into device-specific commands.\n",
      "    *   **Kernel I/O Subsystem:** Provides services like buffering, caching, spooling, error handling, and device reservation.\n",
      "*   **Methods of I/O:** How data is transferred between devices and memory/CPU.\n",
      "    *   **Polling:** The CPU repeatedly checks the status of an I/O device to see if it's ready for data transfer.\n",
      "    *   **Interrupts:** A hardware signal that causes the CPU to stop its current task and handle an event (like I/O completion).\n",
      "    *   **Direct Memory Access (DMA):** A controller that allows I/O devices to transfer data directly to/from main memory without involving the CPU, improving efficiency for large data transfers.\n",
      "*   **Buffering:** Storing data in a temporary memory area during I/O transfers to smooth out speed differences between devices and the CPU.\n",
      "*   **Caching:** Storing copies of data in a faster storage system (cache) for quicker access.\n",
      "*   **Spooling:** Holding data for a device (like a printer) in a buffer until the device is ready, allowing multiple processes to \"print\" concurrently.\n",
      "*   **Disk Structure:** Understanding how data is physically organized on a disk (platters, tracks, sectors).\n",
      "*   **Disk Scheduling:** Algorithms used to minimize the total head movement of the disk arm, thereby reducing access time for disk I/O requests (e.g., FCFS, SSTF, SCAN, C-SCAN, LOOK).\n",
      "\n",
      "---\n",
      "\n",
      "By covering these topics, an Operating System syllabus provides a comprehensive understanding of how modern computers function at a fundamental level, from managing the CPU to handling memory and interacting with peripheral devices.\n",
      "\n",
      "[Agent Summary] Tool used: syllabus_lookup\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of Agentic AI - Agents that can reason and use tools\n",
    "# Modern LangChain uses create_tool_calling_agent or similar functions\n",
    "\n",
    "print(\"=== AGENTS: Reasoning with Tools ===\\n\")\n",
    "\n",
    "# Try to import modern agent creation functions\n",
    "try:\n",
    "    from langchain_core.agents import tool, AgentExecutor\n",
    "    from langchain.agents import create_tool_calling_agent\n",
    "    use_modern_agents = True\n",
    "    print(\"Using modern LangChain agent creation\")\n",
    "except ImportError:\n",
    "    use_modern_agents = False\n",
    "    print(\"Modern agent imports not available, using simple agent demo\")\n",
    "\n",
    "if use_modern_agents:\n",
    "    # Modern approach with tool-calling agent\n",
    "    try:\n",
    "        # Create a tool-calling agent (recommended approach in modern LangChain)\n",
    "        from langchain.agents import create_tool_calling_agent\n",
    "        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "        \n",
    "        # Define the prompt for the agent\n",
    "        agent_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful teaching assistant. You have access to tools that can help you answer questions.\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        \n",
    "        # Create the tool-calling agent\n",
    "        agent = create_tool_calling_agent(\n",
    "            llm=llm,\n",
    "            tools=[syllabus_lookup],\n",
    "            prompt=agent_prompt\n",
    "        )\n",
    "        \n",
    "        # Create the executor that runs the agent loop\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=[syllabus_lookup],\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "        \n",
    "        # Execute the agent\n",
    "        result = agent_executor.invoke({\n",
    "            \"input\": \"What is in the Operating System syllabus? Please use the syllabus lookup tool.\"\n",
    "        })\n",
    "        print(\"\\nAgent Response:\", result.get('output', result))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Tool-calling agent error: {e}\\n\")\n",
    "        use_modern_agents = False\n",
    "\n",
    "if not use_modern_agents:\n",
    "    # Simple Agent Simulation - demonstrates core agent concept when imports fail\n",
    "    print(\"\\nUsing Simplified Agent Demonstration:\\n\")\n",
    "    \n",
    "    class SimpleAgent:\n",
    "        \"\"\"Simple demonstration of an agent that reasons and uses tools\"\"\"\n",
    "        def __init__(self, llm, tools, name=\"TeachingAgent\"):\n",
    "            self.llm = llm\n",
    "            self.tools = {tool.name: tool for tool in tools}\n",
    "            self.name = name\n",
    "            self.memory = []\n",
    "        \n",
    "        def invoke(self, query):\n",
    "            \"\"\"Agent thinks about the problem and decides whether to use tools\"\"\"\n",
    "            print(f\"[{self.name}] Received query: {query['input']}\")\n",
    "            \n",
    "            # Step 1: Reason about the task\n",
    "            reasoning_prompt = f\"\"\"You are {self.name}. Analyze this request:\n",
    "            \n",
    "Request: {query['input']}\n",
    "\n",
    "Available tools: {list(self.tools.keys())}\n",
    "\n",
    "Should you use a tool for this? Respond briefly with YES or NO, then explain your reasoning in 1-2 sentences.\"\"\"\n",
    "            \n",
    "            reasoning_response = llm.invoke(reasoning_prompt)\n",
    "            reasoning_text = reasoning_response.content if hasattr(reasoning_response, 'content') else str(reasoning_response)\n",
    "            print(f\"[Agent Reasoning] {reasoning_text[:200]}...\\n\")\n",
    "            \n",
    "            # Step 2: Decide whether to use a tool\n",
    "            if \"YES\" in reasoning_text.upper():\n",
    "                # Use the available tool\n",
    "                tool_name = list(self.tools.keys())[0]  # Use first available tool\n",
    "                print(f\"[Agent Action] Using tool: {tool_name}\")\n",
    "                \n",
    "                try:\n",
    "                    tool_result = self.tools[tool_name].invoke(query['input'])\n",
    "                    print(f\"[Tool Result] Retrieved syllabus data\\n\")\n",
    "                    \n",
    "                    # Step 3: Synthesize response with tool output\n",
    "                    synthesis_prompt = f\"\"\"Based on this syllabus information:\n",
    "{tool_result}\n",
    "\n",
    "Answer this question: {query['input']}\n",
    "\n",
    "Provide a helpful, educational response.\"\"\"\n",
    "                    \n",
    "                    final_response = llm.invoke(synthesis_prompt)\n",
    "                    final_text = final_response.content if hasattr(final_response, 'content') else str(final_response)\n",
    "                    \n",
    "                    return {\n",
    "                        \"output\": final_text,\n",
    "                        \"tool_used\": tool_name,\n",
    "                        \"reasoning\": reasoning_text\n",
    "                    }\n",
    "                except Exception as tool_error:\n",
    "                    print(f\"[Tool Error] {tool_error}\")\n",
    "                    return {\"output\": \"Could not use tool\", \"error\": str(tool_error)}\n",
    "            else:\n",
    "                # Answer directly without tools\n",
    "                direct_prompt = f\"Question: {query['input']}\\n\\nAnswer based on your knowledge:\"\n",
    "                direct_response = llm.invoke(direct_prompt)\n",
    "                direct_text = direct_response.content if hasattr(direct_response, 'content') else str(direct_response)\n",
    "                \n",
    "                return {\n",
    "                    \"output\": direct_text,\n",
    "                    \"tool_used\": None,\n",
    "                    \"reasoning\": reasoning_text\n",
    "                }\n",
    "    \n",
    "    # Create and run the simple agent\n",
    "    simple_agent = SimpleAgent(llm, [syllabus_lookup], \"TeachingAssistant\")\n",
    "    \n",
    "    try:\n",
    "        result = simple_agent.invoke({\n",
    "            \"input\": \"What topics are covered in Operating System syllabus? Explain the key concepts.\"\n",
    "        })\n",
    "        print(f\"\\n[Final Response]\\n{result['output']}\")\n",
    "        print(f\"\\n[Agent Summary] Tool used: {result.get('tool_used', 'None')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Agent execution error: {e}\")\n",
    "        print(\"Fallback: Calling tool directly...\")\n",
    "        # Ultimate fallback: just use the tool and LLM without agent logic\n",
    "        try:\n",
    "            tool_output = syllabus_lookup.invoke(\"Operating System\")\n",
    "            print(f\"Syllabus retrieved: {tool_output[:200]}...\")\n",
    "        except Exception as tool_error:\n",
    "            print(f\"Tool error: {tool_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c405727",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ RunnableLambda (Custom Logic Inside Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8afc2dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Result:\n",
      "Okay, let's break down the Operating System (OS) in super simple terms.\n",
      "\n",
      "**Imagine your computer is a fancy, high-tech building.**\n",
      "\n",
      "*   It has lots of different rooms (hardware like the processor, memory, hard drive, screen, keyboard).\n",
      "*   It has lots of different services it can offer (running prog\n"
     ]
    }
   ],
   "source": [
    "# Define a custom Python function that will be used in the chain\n",
    "# This function takes LLM output and performs custom processing\n",
    "def summarize(text: str) -> str:\n",
    "    \"\"\"Extract first 300 characters from text\"\"\"\n",
    "    # Use Python slicing to get the first 300 characters\n",
    "    # If text is empty, return a default message\n",
    "    return text[:300] if text else \"No text to summarize\"\n",
    "\n",
    "# Create a RunnableLambda wrapper around the custom function\n",
    "# RunnableLambda converts regular Python functions into runnable components\n",
    "summary_step = RunnableLambda(summarize)\n",
    "\n",
    "# Compose the chain: explain_chain output ‚Üí summarize\n",
    "# The | operator pipes the explain chain result into the summarization function\n",
    "summary_chain = explain_chain | summary_step\n",
    "\n",
    "# Run the summary chain\n",
    "try:\n",
    "    # invoke() passes the input through: topic ‚Üí explain ‚Üí summarize\n",
    "    result = summary_chain.invoke({\"topic\": \"Operating System\"})\n",
    "    print(\"Summary Result:\")\n",
    "    print(result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling for API or execution issues\n",
    "    print(f\"‚ö†Ô∏è Note: Requires valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")\n",
    "    print(f\"This chain successfully combines LLM output with custom Python functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9abff8",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Parallel Chains (Multiple Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6dd3bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Chain Results:\n",
      "Explanation: Imagine your computer is a **bustling hotel**, full of different departments, staff, and guests, all needing things done.\n",
      "\n",
      "The **Operating System (OS)** is like the **super-efficient, always-on manager** of that hotel.\n",
      "\n",
      "Here's what that \"manager\" (your OS) does:\n",
      "\n",
      "1.  **Manages the Hardware (The Hotel's Infrastructure):**\n",
      "    *   **CPU (The Main Chef/Problem-Solver):** The OS makes sure the CPU gets tasks to work on, like cooking meals or solving guest problems. It decides which task gets the CPU's attention at any given moment.\n",
      "    *   **RAM (Temporary Workspace/Rooms):** When you open an app, the OS finds it a \"room\" in RAM to work in. When you close the app, the OS cleans out the room and makes it available for the next guest.\n",
      "    *   **Hard Drive/SSD (Storage Warehouse):** The OS organizes all your files, documents, photos, and programs in the \"warehouse,\" knowing exactly where everything is stored and retrieving it when you need it.\n",
      "    *   **Peripherals (Other Staff/Services):** It handles your keyboard (the receptionist taking notes), mouse (the bellhop guiding you), printer (the document service), and screen (the display board). It translates your actions into commands the hardware understands.\n",
      "\n",
      "2.  **Runs Your Software (Manages the Guests/Activities):**\n",
      "    *   When you click to open a program (like a guest checking in to use the gym or restaurant), the OS loads that program from storage into RAM and gives it \"time\" from the CPU to run.\n",
      "    *   It allows multiple programs to run at once (multitasking), like the hotel manager juggling multiple guest requests ‚Äì ensuring your music keeps playing while you're typing a document and downloading a file.\n",
      "\n",
      "3.  **Provides a User Interface (The Reception Desk/Signs):**\n",
      "    *   This is what you *see* and *interact with*. The desktop, icons, menus, windows, mouse pointer, keyboard input ‚Äì all of this is provided by the OS so you can easily tell the computer what you want to do.\n",
      "\n",
      "4.  **Handles Files (Keeps Records Organized):**\n",
      "    *   It lets you create, save, delete, copy, and move files and folders. It's like the hotel's record-keeping system, ensuring every guest's information, every invoice, and every service request is neatly filed away.\n",
      "\n",
      "5.  **Ensures Security (The Hotel's Security Team):**\n",
      "    *   The OS manages user accounts, passwords, and permissions, deciding who can access what. It also helps protect against unwanted \"intruders\" (malware) and keeps the system stable.\n",
      "\n",
      "**In short:**\n",
      "\n",
      "Without an Operating System, your computer would just be a collection of expensive, inert parts. It's the **foundational software** that makes your computer usable, intelligent, and responsive, allowing you to interact with it and run all your other applications.\n",
      "\n",
      "**Examples:** Windows, macOS, Linux (for desktops/laptops), Android, iOS (for phones/tablets).\n",
      "\n",
      "Quiz: Here are 3 simple quiz questions based on the explanation:\n",
      "\n",
      "1.  What is the main role of an Operating System (OS) in simple terms?\n",
      "2.  Which program is the first to load when you turn on your computer and runs constantly in the background?\n",
      "3.  Name one common example of an Operating System.\n"
     ]
    }
   ],
   "source": [
    "# Create a parallel chain that executes multiple chains simultaneously\n",
    "# Instead of sequential (A ‚Üí B ‚Üí C), parallel chains run A and B at the same time\n",
    "parallel_chain = RunnableParallel(\n",
    "    explanation=explain_chain,      # Run explain_chain and store result as 'explanation' key\n",
    "    quiz=study_helper_chain         # Run study_helper_chain and store result as 'quiz' key\n",
    ")\n",
    "\n",
    "# Run parallel chains\n",
    "try:\n",
    "    # invoke() runs both chains concurrently on the same input\n",
    "    result = parallel_chain.invoke({\"topic\": \"Operating System\"})\n",
    "    \n",
    "    # Print results from both parallel executions\n",
    "    print(\"Parallel Chain Results:\")\n",
    "    print(\"Explanation:\", result.get(\"explanation\", \"\"))  # Get explanation result\n",
    "    print(\"\\nQuiz:\", result.get(\"quiz\", \"\"))               # Get quiz result\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling\n",
    "    print(f\"‚ö†Ô∏è Note: Requires valid GOOGLE_API_KEY\")\n",
    "    print(f\"Error: {type(e).__name__}\")\n",
    "    print(f\"Parallel chains execute multiple outputs simultaneously for efficiency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc254d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£5Ô∏è‚É£ Callbacks (Tracing & Observability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1190677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Callbacks & Observability Demonstration:\n",
      "==================================================\n",
      "\n",
      "üìç Simulating chain execution with tracing:\n",
      "\n",
      "[TraceObserver] Chain Starting\n",
      "[TraceObserver] LLM Starting - Input: {'prompt': 'Explain Operating System in simple terms'}\n",
      "  [LLM Processing...]\n",
      "\n",
      "[TraceObserver] LLM Completed\n",
      "[TraceObserver] Chain Completed - Output received\n",
      "\n",
      "üìã Trace Log (All Observed Events):\n",
      "--------------------------------------------------\n",
      "1. [TraceObserver] Chain Starting\n",
      "2. [TraceObserver] LLM Starting - Input: {'prompt': 'Explain Operating System in simple terms'}\n",
      "3. [TraceObserver] LLM Completed\n",
      "4. [TraceObserver] Chain Completed - Output received\n",
      "\n",
      "‚úÖ Callbacks demonstrate how to trace and observe AI execution\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate callbacks/observability concept using a custom logging handler\n",
    "# Modern LangChain callbacks require advanced setup - using simple demonstration\n",
    "\n",
    "class SimpleCallbackHandler:\n",
    "    \"\"\"Simple callback handler to demonstrate tracing and observability\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Observer\"):\n",
    "        self.name = name\n",
    "        self.events = []\n",
    "    \n",
    "    def on_llm_start(self, serialized, inputs, **kwargs):\n",
    "        \"\"\"Called when LLM execution starts\"\"\"\n",
    "        event = f\"[{self.name}] LLM Starting - Input: {inputs}\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        \"\"\"Called when LLM execution completes\"\"\"\n",
    "        event = f\"[{self.name}] LLM Completed\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        \"\"\"Called when chain execution starts\"\"\"\n",
    "        event = f\"[{self.name}] Chain Starting\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "    \n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        \"\"\"Called when chain execution completes\"\"\"\n",
    "        event = f\"[{self.name}] Chain Completed - Output received\"\n",
    "        print(event)\n",
    "        self.events.append(event)\n",
    "\n",
    "print(\"üìä Callbacks & Observability Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a callback handler instance\n",
    "handler = SimpleCallbackHandler(name=\"TraceObserver\")\n",
    "\n",
    "# Simulate a chain execution with callbacks\n",
    "print(\"\\nüìç Simulating chain execution with tracing:\\n\")\n",
    "handler.on_chain_start({}, {\"input\": \"Explain Operating System\"})\n",
    "handler.on_llm_start({}, {\"prompt\": \"Explain Operating System in simple terms\"})\n",
    "print(\"  [LLM Processing...]\\n\")\n",
    "handler.on_llm_end({\"response\": \"An Operating System manages computer resources...\"})\n",
    "handler.on_chain_end({\"output\": \"Complete response\"})\n",
    "\n",
    "print(\"\\nüìã Trace Log (All Observed Events):\")\n",
    "print(\"-\" * 50)\n",
    "for i, event in enumerate(handler.events, 1):\n",
    "    print(f\"{i}. {event}\")\n",
    "\n",
    "print(\"\\n‚úÖ Callbacks demonstrate how to trace and observe AI execution\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
