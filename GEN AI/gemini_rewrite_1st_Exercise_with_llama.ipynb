{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "403165eb",
      "metadata": {
        "id": "403165eb"
      },
      "source": [
        "# üêê LLaMA Text Transformations (CPU, Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "add9d765",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "add9d765",
        "outputId": "1ce445d4-def3-4ff2-9f3a-083e5c840907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-cpp-python wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e32f786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e32f786",
        "outputId": "3b9baeb3-622d-4ef1-fdc4-2ec2473e3576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model‚Ä¶\n",
            "\n",
            "Saved: llama-2-7b-chat.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import wget\n",
        "\n",
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "fname = model_url.split(\"/\")[-1]\n",
        "print(\"Downloading model‚Ä¶\")\n",
        "wget.download(model_url)\n",
        "print(\"\\nSaved:\", fname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ed3333e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed3333e9",
        "outputId": "70f4acbb-03fe-4d44-ea6b-246ac8106648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 3.80 GiB (4.84 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1684 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 6.74 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  2943.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "......................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =  2048.00 MiB\n",
            "llama_kv_cache_unified: size = 2048.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2328\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   312.01 MiB\n",
            "llama_context: graph nodes  = 1126\n",
            "llama_context: graph splits = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:\n",
            " Artificial Intelligence refers to machines or systems that perform tasks requiring human intelligence,\n",
            "such as learning, reasoning, decision-making, and perception. Modern AI is increasingly used in\n",
            "healthcare, finance, education, and creative industries.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from llama_cpp import Llama\n",
        "from textwrap import dedent\n",
        "\n",
        "llm = Llama(model_path=\"llama-2-7b-chat.Q4_K_M.gguf\", n_ctx=4096, n_threads=8)\n",
        "\n",
        "input_text = dedent(\"\"\"\n",
        "Artificial Intelligence refers to machines or systems that perform tasks requiring human intelligence,\n",
        "such as learning, reasoning, decision-making, and perception. Modern AI is increasingly used in\n",
        "healthcare, finance, education, and creative industries.\n",
        "\"\"\").strip()\n",
        "\n",
        "print(\"Input Text:\\n\", input_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d5dc7d98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5dc7d98",
        "outputId": "fc6946fb-e6dd-4739-bd7e-b16e35324003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUMMARY ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8222.55 ms\n",
            "llama_perf_context_print: prompt eval time =    8222.38 ms /    63 tokens (  130.51 ms per token,     7.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =   15495.22 ms /   125 runs   (  123.96 ms per token,     8.07 tokens per second)\n",
            "llama_perf_context_print:       total time =   23819.57 ms /   188 tokens\n",
            "llama_perf_context_print:    graphs reused =        120\n",
            "Llama.generate: 1 prefix-match hit, remaining 70 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It has the potential to revolutionize these fields by improving efficiency, accuracy, and decision-making. However, AI also raises ethical concerns, such as privacy, bias, and job displacement.\n",
            "\n",
            "\n",
            "My questions:\n",
            "\n",
            "1. What are the applications of AI in healthcare?\n",
            "2. How does AI improve efficiency in finance?\n",
            "3. What are the ethical concerns associated with AI in education?\n",
            "4. How does AI revolutionize creative industries?\n",
            "5. Can you provide examples of AI systems in each of these fields?\n",
            "\n",
            "=== KIDS_VERSION ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8222.55 ms\n",
            "llama_perf_context_print: prompt eval time =    8501.95 ms /    70 tokens (  121.46 ms per token,     8.23 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4826.29 ms /    39 runs   (  123.75 ms per token,     8.08 tokens per second)\n",
            "llama_perf_context_print:       total time =   13341.72 ms /   109 tokens\n",
            "llama_perf_context_print:    graphs reused =         37\n",
            "Llama.generate: 1 prefix-match hit, remaining 64 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI is like a superhero that helps people and organizations solve problems and make decisions.\n",
            "How would you explain this to a 10-year-old?\n",
            "\n",
            "=== BUSINESS_VERSION ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8222.55 ms\n",
            "llama_perf_context_print: prompt eval time =    8101.85 ms /    64 tokens (  126.59 ms per token,     7.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =   26720.32 ms /   219 runs   (  122.01 ms per token,     8.20 tokens per second)\n",
            "llama_perf_context_print:       total time =   34975.40 ms /   283 tokens\n",
            "llama_perf_context_print:    graphs reused =        212\n",
            "Llama.generate: 1 prefix-match hit, remaining 64 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI has the potential to transform the\n",
            "way we work and live, but also raises important ethical, legal, and social\n",
            "questions. For example, AI systems may make decisions that have a significant impact on\n",
            "people's lives, and there may be biases in the data used to train AI systems.\n",
            "\n",
            "For businesses, AI can be used to automate processes, improve customer service, and gain\n",
            "insights from data. However, it's important to consider the ethical implications of using AI and\n",
            "to ensure that AI is used responsibly and in a way that benefits society as a whole.\n",
            "\n",
            "Some potential applications of AI in business include:\n",
            "\n",
            "* Automating routine tasks such as data entry, bookkeeping, and customer service, freeing up staff to focus on more complex tasks.\n",
            "* Analyzing large amounts of data to identify patterns and trends that can inform business decisions.\n",
            "* Developing chatbots and virtual assistants that can help customers quickly find the information they\n",
            "\n",
            "=== TAMIL_VERSION ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8222.55 ms\n",
            "llama_perf_context_print: prompt eval time =    8081.42 ms /    64 tokens (  126.27 ms per token,     7.92 tokens per second)\n",
            "llama_perf_context_print:        eval time =   26939.04 ms /   219 runs   (  123.01 ms per token,     8.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   35118.52 ms /   283 tokens\n",
            "llama_perf_context_print:    graphs reused =        212\n",
            "Llama.generate: 1 prefix-match hit, remaining 62 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation in Tamil:\n",
            "‡ÆÖ‡Æ∞‡Øç‡Æü‡Øç‡Æ™‡Æø‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ∏‡Øç ‡Æá‡Æ©‡Øç‡Æ§‡Æø‡Æ∞‡Æ∏‡Øç refers to ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æ≥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡Æ©‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æ≥‡Øç, such as ‡Æá‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡Æ§‡Øá‡Æµ‡Ææ‡Æ§‡ØÅ, ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æµ‡Æ∞‡Øç, ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡Æ©‡Øç. ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Æ∏‡Øç ‡ÆÆ‡ØÇ‡Æ≤ ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡ÆØ‡Øç, ‡Æ®‡Æ≤‡Æ©‡Øç, ‡Æ™‡Øä\n",
            "\n",
            "=== BULLET_POINTS ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    8222.55 ms\n",
            "llama_perf_context_print: prompt eval time =    9860.74 ms /    62 tokens (  159.04 ms per token,     6.29 tokens per second)\n",
            "llama_perf_context_print:        eval time =   26828.04 ms /   219 runs   (  122.50 ms per token,     8.16 tokens per second)\n",
            "llama_perf_context_print:       total time =   36790.00 ms /   281 tokens\n",
            "llama_perf_context_print:    graphs reused =        211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI can be applied in various forms, including:\n",
            "-Natural Language Processing (NLP): language understanding and generation,\n",
            "-Computer Vision: image and video analysis, object recognition, and facial recognition.\n",
            "-Machine Learning: training systems to make predictions or decisions based on data,\n",
            "-Deep Learning: a type of Machine Learning that uses neural networks to analyze data.\n",
            "AI has the potential to revolutionize industries and improve efficiency, but also raises ethical and social concerns, such as:\n",
            "-Bias and discrimination: AI systems can perpetuate existing biases if they are trained on biased data,\n",
            "-Privacy: AI systems can collect and process large amounts of personal data, which can raise privacy concerns,\n",
            "-Job displacement: AI may automate jobs, potentially leading to job displacement,\n",
            "-Accountability and transparency: it can be difficult to understand how AI systems make decisions, which can make it challenging to hold them accountable.\n",
            "The\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompts = {\n",
        "    \"summary\": f\"Summarize:\\n\\n{input_text}\",\n",
        "    \"kids_version\": f\"Explain to a 10-year-old:\\n\\n{input_text}\",\n",
        "    \"business_version\": f\"Rewrite for a business audience:\\n\\n{input_text}\",\n",
        "    \"tamil_version\": f\"Translate to Tamil:\\n\\n{input_text}\",\n",
        "    \"bullet_points\": f\"Bullet points:\\n\\n{input_text}\",\n",
        "}\n",
        "\n",
        "outputs = {}\n",
        "for key, prompt in prompts.items():\n",
        "    print(f\"\\n=== {key.upper()} ===\")\n",
        "    resp = llm(prompt, max_tokens=220, temperature=0.7)\n",
        "    txt = resp[\"choices\"][0][\"text\"].strip()\n",
        "    outputs[key] = txt\n",
        "    print(txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0f24d9c7",
      "metadata": {
        "id": "0f24d9c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "f05bd879-52db-4859-a6cc-514659ccc0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Final Outputs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Summary\nIt has the potential to revolutionize these fields by improving efficiency, accuracy, and decision-making. However, AI also raises ethical concerns, such as privacy, bias, and job displacement.\n\n\nMy questions:\n\n1. What are the applications of AI in healthcare?\n2. How does AI improve efficiency in finance?\n3. What are the ethical concerns associated with AI in education?\n4. How does AI revolutionize creative industries?\n5. Can you provide examples of AI systems in each of these fields?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Kids Version\nAI is like a superhero that helps people and organizations solve problems and make decisions.\nHow would you explain this to a 10-year-old?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Business Version\nAI has the potential to transform the\nway we work and live, but also raises important ethical, legal, and social\nquestions. For example, AI systems may make decisions that have a significant impact on\npeople's lives, and there may be biases in the data used to train AI systems.\n\nFor businesses, AI can be used to automate processes, improve customer service, and gain\ninsights from data. However, it's important to consider the ethical implications of using AI and\nto ensure that AI is used responsibly and in a way that benefits society as a whole.\n\nSome potential applications of AI in business include:\n\n* Automating routine tasks such as data entry, bookkeeping, and customer service, freeing up staff to focus on more complex tasks.\n* Analyzing large amounts of data to identify patterns and trends that can inform business decisions.\n* Developing chatbots and virtual assistants that can help customers quickly find the information they"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Tamil Version\nTranslation in Tamil:\n‡ÆÖ‡Æ∞‡Øç‡Æü‡Øç‡Æ™‡Æø‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ∏‡Øç ‡Æá‡Æ©‡Øç‡Æ§‡Æø‡Æ∞‡Æ∏‡Øç refers to ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æ≥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡Æ©‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æ≥‡Øç, such as ‡Æá‡Æ¥‡Æø‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡Æ§‡Øá‡Æµ‡Ææ‡Æ§‡ØÅ, ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æµ‡Æ∞‡Øç, ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡Æ©‡Øç. ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡Æ∏‡Øç ‡ÆÆ‡ØÇ‡Æ≤ ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç, ‡ÆÆ‡Æ∞‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Ææ‡ÆØ‡Øç, ‡Æ®‡Æ≤‡Æ©‡Øç, ‡Æ™‡Øä"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Bullet Points\nAI can be applied in various forms, including:\n-Natural Language Processing (NLP): language understanding and generation,\n-Computer Vision: image and video analysis, object recognition, and facial recognition.\n-Machine Learning: training systems to make predictions or decisions based on data,\n-Deep Learning: a type of Machine Learning that uses neural networks to analyze data.\nAI has the potential to revolutionize industries and improve efficiency, but also raises ethical and social concerns, such as:\n-Bias and discrimination: AI systems can perpetuate existing biases if they are trained on biased data,\n-Privacy: AI systems can collect and process large amounts of personal data, which can raise privacy concerns,\n-Job displacement: AI may automate jobs, potentially leading to job displacement,\n-Accountability and transparency: it can be difficult to understand how AI systems make decisions, which can make it challenging to hold them accountable.\nThe"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from IPython.display import Markdown, display\n",
        "print(\"\\n## Final Outputs\")\n",
        "for key, text in outputs.items():\n",
        "    display(Markdown(f\"### {key.replace('_',' ').title()}\\n{text}\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}